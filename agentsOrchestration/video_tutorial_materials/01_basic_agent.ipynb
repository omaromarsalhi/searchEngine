{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Agent Loop Workflow\n",
    "\n",
    "In this notebook, we'll build a basic agent loop workflow that allows an agent to chat with the user and call tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Configuration\n",
    "\n",
    "To represent an agent, we'll use a Pydantic model to capture\n",
    "- the name of the agent\n",
    "- the description of the agent\n",
    "- the system prompt of the agent\n",
    "- the tools that the agent has access to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "from llama_index.core.tools import BaseTool\n",
    "\n",
    "class AgentConfig(BaseModel):\n",
    "    \"\"\"Used to configure an agent.\"\"\"\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    name: str\n",
    "    description: str\n",
    "    system_prompt: str | None = None\n",
    "    tools: list[BaseTool] | None = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this configuration, we can define what a basic agent might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Used to add two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "add_two_numbers_tool = FunctionTool.from_defaults(fn=add_two_numbers)\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    name=\"Addition Agent\",\n",
    "    description=\"Used to add two numbers together.\",\n",
    "    system_prompt=\"You are an agent that adds two numbers together. Do not help the user with anything else.\",\n",
    "    tools=[add_two_numbers_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Definition\n",
    "\n",
    "With our agent configuration, we can define a workflow that uses the configuration to implement a basic agent loop.\n",
    "\n",
    "This workflow will:\n",
    "- initialize the global context with passed in parameters\n",
    "- call an LLM with the system prompt and chat history\n",
    "- parse the tool calls from the LLM response\n",
    "  - if there are no tool calls, the workflow will stop\n",
    "  - if there are tool calls, the workflow will execute the tool calls\n",
    "    - collect the results of the tool calls\n",
    "    - update the chat history with the tool call results\n",
    "    - loop back and call the LLM again with the updated chat history\n",
    "\n",
    "\n",
    "We also allow the user to pass in state that will be included in the system prompt. This is useful for adding in information about the outside world into the agent's context and reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event, Workflow, Context, StartEvent, StopEvent, step\n",
    "from llama_index.core.llms import ChatMessage, LLM\n",
    "from llama_index.core.tools import ToolSelection\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "class LLMCallEvent(Event):\n",
    "    pass\n",
    "\n",
    "class ToolCallEvent(Event):\n",
    "    pass\n",
    "\n",
    "class ToolCallResultEvent(Event):\n",
    "    chat_message: ChatMessage\n",
    "\n",
    "class ProgressEvent(Event):\n",
    "    msg: str\n",
    "    \n",
    "\n",
    "class BasicAgent(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def setup(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> LLMCallEvent:\n",
    "        \"\"\"Sets up the workflow, validates inputs, and stores them in the context.\"\"\"\n",
    "        agent_config = ev.get(\"agent_config\")\n",
    "        user_msg = ev.get(\"user_msg\")\n",
    "        llm: LLM = ev.get(\"llm\", default=OpenAI(model=\"gpt-4o\", temperature=0.3))\n",
    "        chat_history = ev.get(\"chat_history\", default=[])\n",
    "        initial_state = ev.get(\"initial_state\", default={})\n",
    "\n",
    "        if (\n",
    "            user_msg is None\n",
    "            or llm is None\n",
    "            or chat_history is None\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"User message, llm, and chat_history are required!\"\n",
    "            )\n",
    "\n",
    "        if not llm.metadata.is_function_calling_model:\n",
    "            raise ValueError(\"LLM must be a function calling model!\")\n",
    "\n",
    "        await ctx.set(\"agent_config\", agent_config)\n",
    "        await ctx.set(\"llm\", llm)\n",
    "\n",
    "        chat_history.append(ChatMessage(role=\"user\", content=user_msg))\n",
    "        await ctx.set(\"chat_history\", chat_history)\n",
    "\n",
    "        await ctx.set(\"user_state\", initial_state)\n",
    "\n",
    "        return LLMCallEvent()\n",
    "\n",
    "    @step\n",
    "    async def speak_with_agent(\n",
    "        self, ctx: Context, ev: LLMCallEvent\n",
    "    ) -> ToolCallEvent | StopEvent:\n",
    "        \"\"\"Speaks with the active sub-agent and handles tool calls (if any).\"\"\"\n",
    "        # Setup the agent \n",
    "        agent_config: AgentConfig = await ctx.get(\"agent_config\")\n",
    "        chat_history = await ctx.get(\"chat_history\")\n",
    "        llm = await ctx.get(\"llm\")\n",
    "\n",
    "        user_state = await ctx.get(\"user_state\")\n",
    "        user_state_str = \"\\n\".join([f\"{k}: {v}\" for k, v in user_state.items()])\n",
    "        system_prompt = (\n",
    "            agent_config.system_prompt.strip()\n",
    "            + f\"\\n\\nHere is the current user state:\\n{user_state_str}\"\n",
    "        )\n",
    "\n",
    "        llm_input = [ChatMessage(role=\"system\", content=system_prompt)] + chat_history\n",
    "        tools = agent_config.tools\n",
    "\n",
    "        response = await llm.achat_with_tools(tools, chat_history=llm_input)\n",
    "\n",
    "        tool_calls: list[ToolSelection] = llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "        if len(tool_calls) == 0:\n",
    "            chat_history.append(response.message)\n",
    "            await ctx.set(\"chat_history\", chat_history)\n",
    "            return StopEvent(\n",
    "                result={\n",
    "                    \"response\": response.message.content,\n",
    "                    \"chat_history\": chat_history,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        await ctx.set(\"num_tool_calls\", len(tool_calls))\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            ctx.send_event(\n",
    "                ToolCallEvent(tool_call=tool_call, tools=agent_config.tools)\n",
    "            )\n",
    "\n",
    "        chat_history.append(response.message)\n",
    "        await ctx.set(\"chat_history\", chat_history)\n",
    "\n",
    "    @step(num_workers=4)\n",
    "    async def handle_tool_call(\n",
    "        self, ctx: Context, ev: ToolCallEvent\n",
    "    ) -> ToolCallResultEvent:\n",
    "        \"\"\"Handles the execution of a tool call.\"\"\"\n",
    "        tool_call = ev.tool_call\n",
    "        tools_by_name = {tool.metadata.get_name(): tool for tool in ev.tools}\n",
    "\n",
    "        tool_msg = None\n",
    "\n",
    "        tool = tools_by_name.get(tool_call.tool_name)\n",
    "        additional_kwargs = {\n",
    "            \"tool_call_id\": tool_call.tool_id,\n",
    "            \"name\": tool.metadata.get_name(),\n",
    "        }\n",
    "        if not tool:\n",
    "            tool_msg = ChatMessage(\n",
    "                role=\"tool\",\n",
    "                content=f\"Tool {tool_call.tool_name} does not exist\",\n",
    "                additional_kwargs=additional_kwargs,\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            tool_output = await tool.acall(**tool_call.tool_kwargs)\n",
    "\n",
    "            tool_msg = ChatMessage(\n",
    "                role=\"tool\",\n",
    "                content=tool_output.content,\n",
    "                additional_kwargs=additional_kwargs,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            tool_msg = ChatMessage(\n",
    "                role=\"tool\",\n",
    "                content=f\"Encountered error in tool call: {e}\",\n",
    "                additional_kwargs=additional_kwargs,\n",
    "            )\n",
    "\n",
    "        ctx.write_event_to_stream(\n",
    "            ProgressEvent(\n",
    "                msg=f\"Tool {tool_call.tool_name} called with {tool_call.tool_kwargs} returned {tool_msg.content}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return ToolCallResultEvent(chat_message=tool_msg)\n",
    "\n",
    "    @step\n",
    "    async def aggregate_tool_results(\n",
    "        self, ctx: Context, ev: ToolCallResultEvent\n",
    "    ) -> LLMCallEvent:\n",
    "        \"\"\"Collects the results of all tool calls and updates the chat history.\"\"\"\n",
    "        num_tool_calls = await ctx.get(\"num_tool_calls\")\n",
    "        results = ctx.collect_events(ev, [ToolCallResultEvent] * num_tool_calls)\n",
    "        if not results:\n",
    "            return\n",
    "\n",
    "        chat_history = await ctx.get(\"chat_history\")\n",
    "        for result in results:\n",
    "            chat_history.append(result.chat_message)\n",
    "        await ctx.set(\"chat_history\", chat_history)\n",
    "\n",
    "        return LLMCallEvent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out!\n",
    "\n",
    "With our workflow defined, we can now try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool add_two_numbers called with {'a': 10, 'b': 10} returned 20\n",
      "-----------\n",
      "10 + 10 is 20.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0.3)\n",
    "workflow = BasicAgent()\n",
    "\n",
    "handler = workflow.run(\n",
    "    agent_config=agent_config,\n",
    "    user_msg=\"What is 10 + 10?\",\n",
    "    chat_history=[],\n",
    "    initial_state={\"user_name\": \"Logan\"},\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, ProgressEvent):\n",
    "        print(event.msg)\n",
    "\n",
    "print(\"-----------\")\n",
    "\n",
    "final_result = await handler\n",
    "print(final_result[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the chat history is passed in each time, we will need to manage it for the next run. A useful way to do this is with a memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "memory.set(final_result[\"chat_history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets run again with chat history managed by the memory buffer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "I'm here to help with adding two numbers together. Let me know if you need assistance with that!\n"
     ]
    }
   ],
   "source": [
    "handler = workflow.run(\n",
    "    agent_config=agent_config,\n",
    "    user_msg=\"What is the capital of Canada?\",\n",
    "    chat_history=memory.get(),\n",
    "    initial_state={\"user_name\": \"Logan\"},\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, ProgressEvent):\n",
    "        print(event.msg)\n",
    "\n",
    "print(\"-----------\")\n",
    "\n",
    "final_result = await handler\n",
    "print(final_result[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.set(final_result[\"chat_history\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mutli-agent-concierge-QdC2PJgK-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
