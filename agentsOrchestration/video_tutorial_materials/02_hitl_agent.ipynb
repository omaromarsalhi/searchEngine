{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-in-the-Loop (HITL) Agent Loop Workflow\n",
    "\n",
    "In this notebook, we'll extend the basic agent loop workflow to include a human-in-the-loop (HITL) workflow. This will allow us to specify which tools require human review and approval before they are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Configuration\n",
    "\n",
    "In order to capture which tools require human review and approval, we'll need to extend our agent configuration to include a list of tools that require HITL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field,ConfigDict\n",
    "\n",
    "from llama_index.core.tools import BaseTool\n",
    "\n",
    "class AgentConfig(BaseModel):\n",
    "    \"\"\"Used to configure an agent.\"\"\"\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    name: str\n",
    "    description: str\n",
    "    system_prompt: str | None = None\n",
    "    tools: list[BaseTool] | None = None\n",
    "    tools_requiring_human_confirmation: list[str] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can copy our previous agent configuration and extend it to include a list of tools that require HITL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Used to add two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "add_two_numbers_tool = FunctionTool.from_defaults(fn=add_two_numbers)\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    name=\"Addition Agent\",\n",
    "    description=\"Used to add two numbers together.\",\n",
    "    system_prompt=\"You are an agent that adds two numbers together. Do not help the user with anything else.\",\n",
    "    tools=[add_two_numbers_tool],\n",
    "    tools_requiring_human_confirmation=[\"add_two_numbers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Definition\n",
    "\n",
    "With our agent configuration, we can define a workflow that uses the configuration to implement a basic agent loop with HITL.\n",
    "\n",
    "This workflow will:\n",
    "- initialize the global context with passed in parameters\n",
    "- call an LLM with the system prompt and chat history\n",
    "- parse the tool calls from the LLM response\n",
    "  - if there are no tool calls, the workflow will stop\n",
    "  - if there are tool calls, the workflow will execute the tool calls\n",
    "    - if the tool call requires HITL, the workflow will emit an event to transfer control to a human\n",
    "      - the human can approve or reject the tool call\n",
    "    - collect the results of the tool calls\n",
    "    - update the chat history with the tool call results\n",
    "    - loop back and call the LLM again with the updated chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event, Workflow, Context, StartEvent, StopEvent, step\n",
    "from llama_index.core.workflow.events import InputRequiredEvent, HumanResponseEvent\n",
    "from llama_index.core.llms import ChatMessage, LLM\n",
    "from llama_index.core.tools import ToolSelection\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "class LLMCallEvent(Event):\n",
    "    pass\n",
    "\n",
    "class ToolCallEvent(Event):\n",
    "    pass\n",
    "\n",
    "class ToolCallResultEvent(Event):\n",
    "    chat_message: ChatMessage\n",
    "\n",
    "class ProgressEvent(Event):\n",
    "    msg: str\n",
    "\n",
    "# Our two new events!\n",
    "class ToolRequestEvent(InputRequiredEvent):\n",
    "    tool_name: str\n",
    "    tool_id: str\n",
    "    tool_kwargs: dict\n",
    "\n",
    "class ToolApprovedEvent(HumanResponseEvent):\n",
    "    tool_name: str\n",
    "    tool_id: str\n",
    "    tool_kwargs: dict\n",
    "    approved: bool\n",
    "    response: str | None = None\n",
    "    \n",
    "\n",
    "class HITLAgent(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def setup(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> LLMCallEvent:\n",
    "        \"\"\"Sets up the workflow, validates inputs, and stores them in the context.\"\"\"\n",
    "        agent_config = ev.get(\"agent_config\")\n",
    "        user_msg = ev.get(\"user_msg\")\n",
    "        llm: LLM = ev.get(\"llm\", default=OpenAI(model=\"gpt-4o\", temperature=0.3))\n",
    "        chat_history = ev.get(\"chat_history\", default=[])\n",
    "        initial_state = ev.get(\"initial_state\", default={})\n",
    "\n",
    "        if (\n",
    "            user_msg is None\n",
    "            or llm is None\n",
    "            or chat_history is None\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"User message, llm, and chat_history are required!\"\n",
    "            )\n",
    "\n",
    "        if not llm.metadata.is_function_calling_model:\n",
    "            raise ValueError(\"LLM must be a function calling model!\")\n",
    "\n",
    "        await ctx.set(\"agent_config\", agent_config)\n",
    "        await ctx.set(\"llm\", llm)\n",
    "\n",
    "        chat_history.append(ChatMessage(role=\"user\", content=user_msg))\n",
    "        await ctx.set(\"chat_history\", chat_history)\n",
    "\n",
    "        await ctx.set(\"user_state\", initial_state)\n",
    "\n",
    "        return LLMCallEvent()\n",
    "\n",
    "    @step\n",
    "    async def speak_with_agent(\n",
    "        self, ctx: Context, ev: LLMCallEvent\n",
    "    ) -> ToolCallEvent | StopEvent:\n",
    "        \"\"\"Speaks with the active sub-agent and handles tool calls (if any).\"\"\"\n",
    "        # Setup the agent \n",
    "        agent_config: AgentConfig = await ctx.get(\"agent_config\")\n",
    "        chat_history = await ctx.get(\"chat_history\")\n",
    "        llm = await ctx.get(\"llm\")\n",
    "\n",
    "        user_state = await ctx.get(\"user_state\")\n",
    "        user_state_str = \"\\n\".join([f\"{k}: {v}\" for k, v in user_state.items()])\n",
    "        system_prompt = (\n",
    "            agent_config.system_prompt.strip()\n",
    "            + f\"\\n\\nHere is the current user state:\\n{user_state_str}\"\n",
    "        )\n",
    "\n",
    "        llm_input = [ChatMessage(role=\"system\", content=system_prompt)] + chat_history\n",
    "        tools = agent_config.tools\n",
    "\n",
    "        response = await llm.achat_with_tools(tools, chat_history=llm_input)\n",
    "\n",
    "        tool_calls: list[ToolSelection] = llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "        if len(tool_calls) == 0:\n",
    "            chat_history.append(response.message)\n",
    "            await ctx.set(\"chat_history\", chat_history)\n",
    "            return StopEvent(\n",
    "                result={\n",
    "                    \"response\": response.message.content,\n",
    "                    \"chat_history\": chat_history,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        await ctx.set(\"num_tool_calls\", len(tool_calls))\n",
    "\n",
    "        # New logic for HITL\n",
    "        for tool_call in tool_calls:\n",
    "            if tool_call.tool_name in agent_config.tools_requiring_human_confirmation:\n",
    "                ctx.write_event_to_stream(\n",
    "                    ToolRequestEvent(\n",
    "                        prefix=f\"Tool {tool_call.tool_name} requires human approval.\",\n",
    "                        tool_name=tool_call.tool_name,\n",
    "                        tool_kwargs=tool_call.tool_kwargs,\n",
    "                        tool_id=tool_call.tool_id,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                ctx.send_event(\n",
    "                    ToolCallEvent(tool_call=tool_call, tools=agent_config.tools)\n",
    "                )\n",
    "\n",
    "        chat_history.append(response.message)\n",
    "        await ctx.set(\"chat_history\", chat_history)\n",
    "    \n",
    "    @step\n",
    "    async def handle_tool_approval(\n",
    "        self, ctx: Context, ev: ToolApprovedEvent\n",
    "    ) -> ToolCallEvent | ToolCallResultEvent:\n",
    "        \"\"\"Handles the approval or rejection of a tool call.\"\"\"\n",
    "        if ev.approved:\n",
    "            agent_config = await ctx.get(\"agent_config\")\n",
    "            return ToolCallEvent(\n",
    "                tools=agent_config.tools,\n",
    "                tool_call=ToolSelection(\n",
    "                    tool_id=ev.tool_id,\n",
    "                    tool_name=ev.tool_name,\n",
    "                    tool_kwargs=ev.tool_kwargs,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            return ToolCallResultEvent(\n",
    "                chat_message=ChatMessage(\n",
    "                    role=\"tool\",\n",
    "                    content=ev.response or \"Tool call was not approved.\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "    @step(num_workers=4)\n",
    "    async def handle_tool_call(\n",
    "        self, ctx: Context, ev: ToolCallEvent\n",
    "    ) -> ToolCallResultEvent:\n",
    "        \"\"\"Handles the execution of a tool call.\"\"\"\n",
    "        tool_call = ev.tool_call\n",
    "        tools_by_name = {tool.metadata.get_name(): tool for tool in ev.tools}\n",
    "\n",
    "        tool_msg = None\n",
    "\n",
    "        tool = tools_by_name.get(tool_call.tool_name)\n",
    "        additional_kwargs = {\n",
    "            \"tool_call_id\": tool_call.tool_id,\n",
    "            \"name\": tool.metadata.get_name(),\n",
    "        }\n",
    "        if not tool:\n",
    "            tool_msg = ChatMessage(\n",
    "                role=\"tool\",\n",
    "                content=f\"Tool {tool_call.tool_name} does not exist\",\n",
    "                additional_kwargs=additional_kwargs,\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            tool_output = await tool.acall(**tool_call.tool_kwargs)\n",
    "\n",
    "            tool_msg = ChatMessage(\n",
    "                role=\"tool\",\n",
    "                content=tool_output.content,\n",
    "                additional_kwargs=additional_kwargs,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            tool_msg = ChatMessage(\n",
    "                role=\"tool\",\n",
    "                content=f\"Encountered error in tool call: {e}\",\n",
    "                additional_kwargs=additional_kwargs,\n",
    "            )\n",
    "\n",
    "        ctx.write_event_to_stream(\n",
    "            ProgressEvent(\n",
    "                msg=f\"Tool {tool_call.tool_name} called with {tool_call.tool_kwargs} returned {tool_msg.content}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return ToolCallResultEvent(chat_message=tool_msg)\n",
    "\n",
    "    @step\n",
    "    async def aggregate_tool_results(\n",
    "        self, ctx: Context, ev: ToolCallResultEvent\n",
    "    ) -> LLMCallEvent:\n",
    "        \"\"\"Collects the results of all tool calls and updates the chat history.\"\"\"\n",
    "        num_tool_calls = await ctx.get(\"num_tool_calls\")\n",
    "        results = ctx.collect_events(ev, [ToolCallResultEvent] * num_tool_calls)\n",
    "        if not results:\n",
    "            return\n",
    "\n",
    "        chat_history = await ctx.get(\"chat_history\")\n",
    "        for result in results:\n",
    "            chat_history.append(result.chat_message)\n",
    "        await ctx.set(\"chat_history\", chat_history)\n",
    "\n",
    "        return LLMCallEvent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out!\n",
    "\n",
    "With our workflow defined, we can now try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool add_two_numbers requires human approval. Approving!\n",
      "Tool add_two_numbers called with {'a': 10, 'b': 10} returned 20\n",
      "-----------\n",
      "10 + 10 is 20.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0.3)\n",
    "workflow = HITLAgent()\n",
    "\n",
    "handler = workflow.run(\n",
    "    agent_config=agent_config,\n",
    "    user_msg=\"What is 10 + 10?\",\n",
    "    chat_history=[],\n",
    "    initial_state={\"user_name\": \"Logan\"},\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, ProgressEvent):\n",
    "        print(event.msg)\n",
    "    elif isinstance(event, ToolRequestEvent):\n",
    "        print(f\"Tool {event.tool_name} requires human approval. Approving!\")\n",
    "        # TODO: Implement your own logic to approve or reject the tool call\n",
    "        # TODO: Try to reject the tool call and see what happens!\n",
    "        handler.ctx.send_event(ToolApprovedEvent(\n",
    "            approved=True,\n",
    "            tool_name=event.tool_name,\n",
    "            tool_id=event.tool_id,\n",
    "            tool_kwargs=event.tool_kwargs,\n",
    "        ))\n",
    "\n",
    "print(\"-----------\")\n",
    "\n",
    "final_result = await handler\n",
    "print(final_result[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the chat history is passed in each time, we will need to manage it for the next run. A useful way to do this is with a memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "memory.set(final_result[\"chat_history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets run again with chat history managed by the memory buffer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "I'm here to help with adding two numbers together.\n"
     ]
    }
   ],
   "source": [
    "handler = workflow.run(\n",
    "    agent_config=agent_config,\n",
    "    user_msg=\"What is the capital of Canada?\",\n",
    "    chat_history=memory.get(),\n",
    "    initial_state={\"user_name\": \"Logan\"},\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, ProgressEvent):\n",
    "        print(event.msg)\n",
    "\n",
    "print(\"-----------\")\n",
    "\n",
    "final_result = await handler\n",
    "print(final_result[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.set(final_result[\"chat_history\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mutli-agent-concierge-QdC2PJgK-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
